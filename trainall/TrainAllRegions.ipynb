{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for All Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions=['5j','INC_ge6j3b','INC_ge6jge4b']\n",
    "invertTestTrain=False\n",
    "hpmasses=[200,225,250,275,300,350,400,500,600,700,800,900,1000,1200,1400,1600,1800,2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! rm -rf HplusML pandas_INC_*.h5\n",
    "#! wget https://jglatzer.web.cern.ch/jglatzer/hpml/pandas_INC_ge6j3b.h5\n",
    "#! wget https://jglatzer.web.cern.ch/jglatzer/hpml/pandas_INC_ge6jge4b.h5\n",
    "#! wget https://jglatzer.web.cern.ch/jglatzer/hpml/pandas_INC_5j3b.h5\n",
    "#! wget https://jglatzer.web.cern.ch/jglatzer/hpml/pandas_INC_5jge4b.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BDTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import HpTrainingFrame\n",
    "import HpAlgorithms\n",
    "import HpHyperParameterOptimisation\n",
    "from joblib import dump\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from HpMLWeightTransformer import MultiSBWeightsScaler\n",
    "from HpMLFeatureNormalisation import WeightedStandardScaler\n",
    "from HpMLPipeline import PipelineWithWeights\n",
    "import HpKerasUtils\n",
    "import numpy as np\n",
    "\n",
    "def getCallbacks(model):\n",
    "    \"\"\" standard callbacks for Keras \"\"\"\n",
    "    return [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ModelCheckpoint(filepath='model_nn_'+str(model.configuration)+\"_dropout\"+str(model.dropout)+\"_l2threshold\"+str(model.l2threshold)+\".hdf5\",\n",
    "                        monitor='val_loss',\n",
    "                        save_best_only=True)\n",
    "      ]\n",
    "\n",
    "def trainBDTandNN(region, hpmass, invertTestTrain, df_mc):\n",
    "    htf=HpTrainingFrame.HpTrainingFrame(df_mc)\n",
    "    X_train, X_test, X_eval, y_train, y_test,y_eval, w_train, w_test, w_eval=htf.prepare(hpmass=hpmass, region=None)\n",
    "    \n",
    "    datalabel=\"train2mod0\"\n",
    "    if invertTestTrain:\n",
    "        datalabel=\"train2mod1\"\n",
    "        X_tmp=X_train\n",
    "        y_tmp=y_train\n",
    "        w_tmp=w_train\n",
    "        X_train=X_test\n",
    "        y_train=y_test\n",
    "        w_train=w_test\n",
    "        X_test=X_tmp\n",
    "        y_test=y_tmp\n",
    "        w_test=w_tmp\n",
    "    \n",
    "    # BDT\n",
    "    \"\"\"clf=HpAlgorithms.getGradientBDTClassifier()\n",
    "    opt=HpHyperParameterOptimisation.HpOptimise('StandardBDT_'+region+'_Hp'+str(hpmass)+'_'+datalabel,clf,X_train,y_train,w_train,X_test,y_test,w_test)\n",
    "    test,train=opt.trainAndTest(silent=True)\n",
    "    dump(clf, 'models/standardBDT_'+str(hpmass)+'_'+region+'_'+datalabel+'.joblib') \"\"\"\n",
    "\n",
    "    # NN\n",
    "    msb=MultiSBWeightsScaler(backgroundclass=0)\n",
    "    wss=WeightedStandardScaler()\n",
    "    steps=[(\"msb\",msb),(\"wss\",wss)]\n",
    "    pipe=PipelineWithWeights(steps)\n",
    "    pipe.fit(X_train.values,y_train.values, sample_weight=w_train.values)\n",
    "    dump(pipe, 'models/standardNN_'+str(hpmass)+'_'+region+'_'+datalabel+'_pipe.joblib')\n",
    "    X_train,y_train,w_train=pipe.transform(X_train.values,y_train.values, sample_weight=w_train.values)\n",
    "    X_test,y_test,w_test=pipe.transform(X_test.values,y_test.values, sample_weight=w_test.values)\n",
    "    #nonzerovariance=np.where(wss.scale_!=0)\n",
    "    #X_train=X_train[:,nonzerovariance[0]]    \n",
    "    #print(X_train.shape, X_test.shape)\n",
    "    \n",
    "    model=HpKerasUtils.HpFeedForwardModel(configuration=[64,64],dropout=0.1, verbose=True, input_dim=X_train.shape[1])\n",
    "    result=model.train((X_train, y_train, w_train),(X_test, y_test, w_test), patience=5,callbacks=getCallbacks(model))\n",
    "    arch_file=open('models/standardNN_'+str(hpmass)+'_'+region+'_'+datalabel+'_architecture.h5','w')\n",
    "    arch_file.write(model.model.to_json())\n",
    "    arch_file.close()\n",
    "    model.model.save_weights('models/standardNN_'+str(hpmass)+'_'+region+'_'+datalabel+'_weights.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HpMLMTL import HpMTLBackgroundAugmenter\n",
    "from HpMLUtils import FeatureDivider\n",
    "\n",
    "def trainMassParameterisedBDTandNN(region, invertTestTrain, df_mc):\n",
    "\n",
    "    htf=HpTrainingFrame.HpTrainingFrame(df_mc)\n",
    "    X_train, X_test, X_eval, y_train, y_test,y_eval, w_train, w_test, w_eval=htf.prepare(hpmass=\"multi\",region=None,addMass=True)\n",
    "\n",
    "    datalabel=\"train2mod0\"\n",
    "    if invertTestTrain:\n",
    "      datalabel=\"train2mod1\"\n",
    "      X_tmp=X_train\n",
    "      y_tmp=y_train\n",
    "      w_tmp=w_train\n",
    "      X_train=X_test\n",
    "      y_train=y_test\n",
    "      w_train=w_test\n",
    "      X_test=X_tmp\n",
    "      y_test=y_tmp\n",
    "      w_test=w_tmp\n",
    "\n",
    "    y_train=X_train.hpmass.copy()\n",
    "    y_test=X_test.hpmass.copy()\n",
    "\n",
    "    #BDT\n",
    "    \"\"\"print(\"BDT1\")\n",
    "    msb=MultiSBWeightsScaler(backgroundclass=-1)\n",
    "    aug=HpMTLBackgroundAugmenter(backgroundclass=-1)\n",
    "    fd=FeatureDivider(\"hpmass\")\n",
    "    steps=[(\"msb\",msb),(\"aug\",aug),(\"fd\",fd)]\n",
    "    pipe=PipelineWithWeights(steps)\n",
    "    pipe.fit(X_train,X_train.hpmass, sample_weight=w_train)\n",
    "    dump(pipe, 'models/massparameterisedBDT_'+region+'_'+datalabel+'_pipe.joblib')\n",
    "    X_tr,y_tr,w_tr=pipe.transform(X_train,y_train, sample_weight=w_train)\n",
    "    X_ts,y_ts,w_ts=pipe.transform(X_test,y_test, sample_weight=w_test)\n",
    "    y_tr=(y_tr>0)\n",
    "    y_ts=(y_ts>0)\n",
    "    print(\"BDT1a\")\n",
    "    clf=HpAlgorithms.getGradientBDTClassifier(options = {'n_estimators': 200, 'learning_rate': 0.1}) #let's get away from the default trees to get a better performance\n",
    "    clf.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "    print(\"BDT1b\")\n",
    "    dump(clf, 'models/massparameterisedBDT_'+region+'_'+datalabel+'_bdt.joblib') \"\"\"\n",
    "    \n",
    "    #NN 1\n",
    "    \"\"\"print(\"NN1\")\n",
    "    msb=MultiSBWeightsScaler(backgroundclass=-1)\n",
    "    aug=HpMTLBackgroundAugmenter(backgroundclass=-1)\n",
    "    fd=FeatureDivider(\"hpmass\")\n",
    "    wss=WeightedStandardScaler()\n",
    "    steps=[(\"msb\",msb),(\"aug\",aug),(\"fd\",fd),(\"wss\",wss)]\n",
    "    pipe=PipelineWithWeights(steps)\n",
    "    pipe.fit(X_train.values,X_train.hpmass.values, sample_weight=w_train.values)\n",
    "    dump(pipe, 'models/massparameterisedNN1_'+region+'_'+datalabel+'_pipe.joblib')\n",
    "    X_tr,y_tr,w_tr=pipe.transform(X_train.values,y_train.values, sample_weight=w_train.values)\n",
    "    X_ts,y_ts,w_ts=pipe.transform(X_test.values,y_test.values, sample_weight=w_test.values)\n",
    "    y_tr=(y_tr>0)\n",
    "    y_ts=(y_ts>0)\n",
    "    modelNN1=HpKerasUtils.HpFeedForwardModel(configuration=[64,64],dropout=0.1, verbose=True, input_dim=X_train.shape[1])\n",
    "    print(\"NN1a\")\n",
    "    resultNN1=modelNN1.train((X_tr, y_tr, w_tr),(X_ts, y_ts, w_ts), patience=5,callbacks=getCallbacks(modelNN1))\n",
    "    print(\"NN1b\")\n",
    "    arch_file=open('models/massparameterisedNN1_'+region+'_'+datalabel+'_architecture.h5','w')\n",
    "    arch_file.write(modelNN1.model.to_json())\n",
    "    arch_file.close()\n",
    "    modelNN1.model.save_weights('models/massparameterisedNN1_'+region+'_'+datalabel+'_weights.h5')\"\"\"\n",
    "\n",
    "    #NN 3\n",
    "    print(\"NN3\")\n",
    "    from HpMLWeightTransformer import WeightsMultiplier\n",
    "    scales={\n",
    "      200:16.,\n",
    "      225:8.,\n",
    "      250:8.,\n",
    "      275:8.,\n",
    "      300:16./3.,\n",
    "      350:4.,\n",
    "      400:8./3.,\n",
    "      500:2.,\n",
    "      600:2.,\n",
    "      700:2.,\n",
    "      800:2.,\n",
    "      900:2.,\n",
    "      1000:4./3.,\n",
    "      1200:1.,\n",
    "      1400:1.,\n",
    "      1600:1.2,\n",
    "      1800:1.5,\n",
    "      2000:4.,\n",
    "    }\n",
    "    msb=MultiSBWeightsScaler(backgroundclass=-1)\n",
    "    scl=WeightsMultiplier(scales=scales,backgroundclass=-1)\n",
    "    aug=HpMTLBackgroundAugmenter(backgroundclass=-1)\n",
    "    fd=FeatureDivider(\"hpmass\")\n",
    "    wss=WeightedStandardScaler()\n",
    "    steps=[(\"msb\",msb),(\"scl\",scl),(\"aug\",aug),(\"fd\",fd),(\"wss\",wss)]\n",
    "    pipe=PipelineWithWeights(steps)\n",
    "    pipe.fit(X_train,X_train.hpmass, sample_weight=w_train)\n",
    "    dump(pipe, 'models/massparameterisedNN3_'+region+'_'+datalabel+'_pipe.joblib')\n",
    "    X_tr,y_tr,w_tr=pipe.transform(X_train,y_train, sample_weight=w_train)\n",
    "    X_ts,y_ts,w_ts=pipe.transform(X_test,y_test, sample_weight=w_test)\n",
    "    y_tr=(y_tr>0)\n",
    "    y_ts=(y_ts>0)\n",
    "    modelNN3=HpKerasUtils.HpFeedForwardModel(configuration=[64,64],dropout=0.1, verbose=True, input_dim=X_tr.shape[1])\n",
    "    resultNN3=modelNN3.train((X_tr.values, y_tr.values, w_tr.values),(X_ts.values, y_ts.values, w_ts.values), patience=5,callbacks=getCallbacks(modelNN3))\n",
    "    arch_file=open('models/massparameterisedNN3_'+region+'_'+datalabel+'_architecture.h5','w')\n",
    "    arch_file.write(modelNN3.model.to_json())\n",
    "    arch_file.close()\n",
    "    modelNN3.model.save_weights('models/massparameterisedNN3_'+region+'_'+datalabel+'_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region= 5j invertTestTrain= False H+ mass=all\n",
      "NN3\n",
      "(3401795, 27) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26]\n",
      "(3401795, 27) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26]\n",
      "(3413977, 27) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26]\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                1792      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 6,017\n",
      "Trainable params: 6,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 3401795 samples, validate on 3413977 samples\n",
      "Epoch 1/100\n",
      "3401795/3401795 [==============================] - 268s 79us/step - loss: 2.8183e-07 - binary_accuracy: 0.7081 - val_loss: 2.7099e-07 - val_binary_accuracy: 0.7243\n",
      "Epoch 2/100\n",
      "3401795/3401795 [==============================] - 253s 74us/step - loss: 2.7316e-07 - binary_accuracy: 0.7190 - val_loss: 2.6924e-07 - val_binary_accuracy: 0.7419\n",
      "Epoch 3/100\n",
      "3401795/3401795 [==============================] - 247s 73us/step - loss: 2.7128e-07 - binary_accuracy: 0.7231 - val_loss: 2.6794e-07 - val_binary_accuracy: 0.7327\n",
      "Epoch 4/100\n",
      "3401795/3401795 [==============================] - 247s 73us/step - loss: 2.7024e-07 - binary_accuracy: 0.7238 - val_loss: 2.6741e-07 - val_binary_accuracy: 0.7175\n",
      "Epoch 5/100\n",
      "3401795/3401795 [==============================] - 246s 72us/step - loss: 2.6929e-07 - binary_accuracy: 0.7249 - val_loss: 2.6678e-07 - val_binary_accuracy: 0.7175\n",
      "Epoch 6/100\n",
      "3401795/3401795 [==============================] - 250s 74us/step - loss: 2.6846e-07 - binary_accuracy: 0.7267 - val_loss: 2.6639e-07 - val_binary_accuracy: 0.7185\n",
      "Epoch 7/100\n",
      "3401795/3401795 [==============================] - 254s 75us/step - loss: 2.6816e-07 - binary_accuracy: 0.7261 - val_loss: 2.6628e-07 - val_binary_accuracy: 0.7144\n",
      "Epoch 8/100\n",
      "3401795/3401795 [==============================] - 255s 75us/step - loss: 2.6785e-07 - binary_accuracy: 0.7277 - val_loss: 2.6607e-07 - val_binary_accuracy: 0.7325\n",
      "Epoch 9/100\n",
      "2016650/3401795 [================>.............] - ETA: 1:20 - loss: 2.6735e-07 - binary_accuracy: 0.7276"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for region in regions:\n",
    "    if region==\"5j\":\n",
    "        df_5j3b=pd.read_hdf('pandas_INC_5j3b.h5', 'INC_5j3b')\n",
    "        df_5j4b=pd.read_hdf('pandas_INC_5jge4b.h5', 'INC_5jge4b')\n",
    "        df_mc=pd.concat([df_5j3b,df_5j4b], ignore_index=False)\n",
    "    else:\n",
    "        df_mc=pd.read_hdf('pandas_'+region+'.h5', region)\n",
    "    for invertTestTrain in [False,True]: #False=2mod0, True=2mod1\n",
    "        #for hpmass in hpmasses:\n",
    "        #    print(\"Region=\",region,\"invertTestTrain=\", invertTestTrain,\"H+ mass=\",hpmass)\n",
    "        #    trainBDTandNN(region, hpmass, invertTestTrain, df_mc)\n",
    "        print(\"Region=\",region,\"invertTestTrain=\", invertTestTrain,\"H+ mass=all\")\n",
    "        trainMassParameterisedBDTandNN(region, invertTestTrain, df_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5j3b.iloc[:,[1,2]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5j3b.iloc[:,[2,1]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
