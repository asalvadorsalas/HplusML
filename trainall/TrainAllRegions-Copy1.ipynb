{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training for All Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions=['5j','INC_ge6j3b','INC_ge6jge4b']\n",
    "regions=['INC_5j3b','INC_5jge4b','INC_ge6j3b','INC_ge6jge4b']\n",
    "invertTestTrain=False\n",
    "hpmasses=[200,225,250,275,300,350,400,500,600,700,800,900,1000,1200,1400,1600,1800,2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! rm -rf HplusML pandas_INC_*.h5\n",
    "#! wget https://jglatzer.web.cern.ch/jglatzer/hpml/pandas_INC_ge6j3b.h5\n",
    "#! wget https://jglatzer.web.cern.ch/jglatzer/hpml/pandas_INC_ge6jge4b.h5\n",
    "#! wget https://jglatzer.web.cern.ch/jglatzer/hpml/pandas_INC_5j3b.h5\n",
    "#! wget https://jglatzer.web.cern.ch/jglatzer/hpml/pandas_INC_5jge4b.h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train BDTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import HpTrainingFrame\n",
    "import HpAlgorithms\n",
    "import HpHyperParameterOptimisation\n",
    "from joblib import dump\n",
    "from keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from HpMLWeightTransformer import MultiSBWeightsScaler\n",
    "from HpMLFeatureNormalisation import WeightedStandardScaler\n",
    "from HpMLPipeline import PipelineWithWeights\n",
    "import HpKerasUtils\n",
    "import numpy as np\n",
    "\n",
    "def getCallbacks(model):\n",
    "    \"\"\" standard callbacks for Keras \"\"\"\n",
    "    return [\n",
    "        EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "        ModelCheckpoint(filepath='model_nn_'+str(model.configuration)+\"_dropout\"+str(model.dropout)+\"_l2threshold\"+str(model.l2threshold)+\".hdf5\",\n",
    "                        monitor='val_loss',\n",
    "                        save_best_only=True)\n",
    "      ]\n",
    "\n",
    "def trainBDTandNN(region, hpmass, invertTestTrain, df_mc):\n",
    "    htf=HpTrainingFrame.HpTrainingFrame(df_mc)\n",
    "    X_train, X_test, X_eval, y_train, y_test,y_eval, w_train, w_test, w_eval=htf.prepare(hpmass=hpmass, region=None)\n",
    "    \n",
    "    datalabel=\"train2mod0\"\n",
    "    if invertTestTrain:\n",
    "        datalabel=\"train2mod1\"\n",
    "        X_tmp=X_train\n",
    "        y_tmp=y_train\n",
    "        w_tmp=w_train\n",
    "        X_train=X_test\n",
    "        y_train=y_test\n",
    "        w_train=w_test\n",
    "        X_test=X_tmp\n",
    "        y_test=y_tmp\n",
    "        w_test=w_tmp\n",
    "    \n",
    "    # BDT\n",
    "    \"\"\"clf=HpAlgorithms.getGradientBDTClassifier()\n",
    "    opt=HpHyperParameterOptimisation.HpOptimise('StandardBDT_'+region+'_Hp'+str(hpmass)+'_'+datalabel,clf,X_train,y_train,w_train,X_test,y_test,w_test)\n",
    "    test,train=opt.trainAndTest(silent=True)\n",
    "    dump(clf, 'models/standardBDT_'+str(hpmass)+'_'+region+'_'+datalabel+'.joblib') \"\"\"\n",
    "\n",
    "    # NN\n",
    "    msb=MultiSBWeightsScaler(backgroundclass=0)\n",
    "    wss=WeightedStandardScaler()\n",
    "    steps=[(\"msb\",msb),(\"wss\",wss)]\n",
    "    pipe=PipelineWithWeights(steps)\n",
    "    pipe.fit(X_train.values,y_train.values, sample_weight=w_train.values)\n",
    "    dump(pipe, 'models/standardNN_'+str(hpmass)+'_'+region+'_'+datalabel+'_pipe.joblib')\n",
    "    X_train,y_train,w_train=pipe.transform(X_train.values,y_train.values, sample_weight=w_train.values)\n",
    "    X_test,y_test,w_test=pipe.transform(X_test.values,y_test.values, sample_weight=w_test.values)\n",
    "    #nonzerovariance=np.where(wss.scale_!=0)\n",
    "    #X_train=X_train[:,nonzerovariance[0]]    \n",
    "    #print(X_train.shape, X_test.shape)\n",
    "    \n",
    "    model=HpKerasUtils.HpFeedForwardModel(configuration=[64,64],dropout=0.1, verbose=True, input_dim=X_train.shape[1])\n",
    "    result=model.train((X_train, y_train, w_train),(X_test, y_test, w_test), patience=5,callbacks=getCallbacks(model))\n",
    "    arch_file=open('models/standardNN_'+str(hpmass)+'_'+region+'_'+datalabel+'_architecture.h5','w')\n",
    "    arch_file.write(model.model.to_json())\n",
    "    arch_file.close()\n",
    "    model.model.save_weights('models/standardNN_'+str(hpmass)+'_'+region+'_'+datalabel+'_weights.h5')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from HpMLMTL import HpMTLBackgroundAugmenter\n",
    "from HpMLUtils import FeatureDivider\n",
    "\n",
    "def trainMassParameterisedBDTandNN(region, invertTestTrain, df_mc):\n",
    "\n",
    "    htf=HpTrainingFrame.HpTrainingFrame(df_mc)\n",
    "    X_train, X_test, X_eval, y_train, y_test,y_eval, w_train, w_test, w_eval=htf.prepare(hpmass=\"multi\",region=None,addMass=True)\n",
    "\n",
    "    datalabel=\"train2mod0\"\n",
    "    if invertTestTrain:\n",
    "      datalabel=\"train2mod1\"\n",
    "      X_tmp=X_train\n",
    "      y_tmp=y_train\n",
    "      w_tmp=w_train\n",
    "      X_train=X_test\n",
    "      y_train=y_test\n",
    "      w_train=w_test\n",
    "      X_test=X_tmp\n",
    "      y_test=y_tmp\n",
    "      w_test=w_tmp\n",
    "\n",
    "    y_train=X_train.hpmass.copy()\n",
    "    y_test=X_test.hpmass.copy()\n",
    "\n",
    "    #BDT\n",
    "    \"\"\"print(\"BDT1\")\n",
    "    msb=MultiSBWeightsScaler(backgroundclass=-1)\n",
    "    aug=HpMTLBackgroundAugmenter(backgroundclass=-1)\n",
    "    fd=FeatureDivider(\"hpmass\")\n",
    "    steps=[(\"msb\",msb),(\"aug\",aug),(\"fd\",fd)]\n",
    "    pipe=PipelineWithWeights(steps)\n",
    "    pipe.fit(X_train,X_train.hpmass, sample_weight=w_train)\n",
    "    dump(pipe, 'models/massparameterisedBDT_'+region+'_'+datalabel+'_pipe.joblib')\n",
    "    X_tr,y_tr,w_tr=pipe.transform(X_train,y_train, sample_weight=w_train)\n",
    "    X_ts,y_ts,w_ts=pipe.transform(X_test,y_test, sample_weight=w_test)\n",
    "    y_tr=(y_tr>0)\n",
    "    y_ts=(y_ts>0)\n",
    "    print(\"BDT1a\")\n",
    "    clf=HpAlgorithms.getGradientBDTClassifier(options = {'n_estimators': 200, 'learning_rate': 0.1}) #let's get away from the default trees to get a better performance\n",
    "    clf.fit(X_tr, y_tr, sample_weight=w_tr)\n",
    "    print(\"BDT1b\")\n",
    "    dump(clf, 'models/massparameterisedBDT_'+region+'_'+datalabel+'_bdt.joblib') \"\"\"\n",
    "    \n",
    "    #NN 1\n",
    "    \"\"\"print(\"NN1\")\n",
    "    msb=MultiSBWeightsScaler(backgroundclass=-1)\n",
    "    aug=HpMTLBackgroundAugmenter(backgroundclass=-1)\n",
    "    fd=FeatureDivider(\"hpmass\")\n",
    "    wss=WeightedStandardScaler()\n",
    "    steps=[(\"msb\",msb),(\"aug\",aug),(\"fd\",fd),(\"wss\",wss)]\n",
    "    pipe=PipelineWithWeights(steps)\n",
    "    pipe.fit(X_train.values,X_train.hpmass.values, sample_weight=w_train.values)\n",
    "    dump(pipe, 'models/massparameterisedNN1_'+region+'_'+datalabel+'_pipe.joblib')\n",
    "    X_tr,y_tr,w_tr=pipe.transform(X_train.values,y_train.values, sample_weight=w_train.values)\n",
    "    X_ts,y_ts,w_ts=pipe.transform(X_test.values,y_test.values, sample_weight=w_test.values)\n",
    "    y_tr=(y_tr>0)\n",
    "    y_ts=(y_ts>0)\n",
    "    modelNN1=HpKerasUtils.HpFeedForwardModel(configuration=[64,64],dropout=0.1, verbose=True, input_dim=X_train.shape[1])\n",
    "    print(\"NN1a\")\n",
    "    resultNN1=modelNN1.train((X_tr, y_tr, w_tr),(X_ts, y_ts, w_ts), patience=5,callbacks=getCallbacks(modelNN1))\n",
    "    print(\"NN1b\")\n",
    "    arch_file=open('models/massparameterisedNN1_'+region+'_'+datalabel+'_architecture.h5','w')\n",
    "    arch_file.write(modelNN1.model.to_json())\n",
    "    arch_file.close()\n",
    "    modelNN1.model.save_weights('models/massparameterisedNN1_'+region+'_'+datalabel+'_weights.h5')\"\"\"\n",
    "\n",
    "    #NN 3\n",
    "    print(\"NN3\")\n",
    "    from HpMLWeightTransformer import WeightsMultiplier\n",
    "    scales={\n",
    "      200:16.,\n",
    "      225:8.,\n",
    "      250:8.,\n",
    "      275:8.,\n",
    "      300:16./3.,\n",
    "      350:4.,\n",
    "      400:8./3.,\n",
    "      500:2.,\n",
    "      600:2.,\n",
    "      700:2.,\n",
    "      800:2.,\n",
    "      900:2.,\n",
    "      1000:4./3.,\n",
    "      1200:1.,\n",
    "      1400:1.,\n",
    "      1600:1.2,\n",
    "      1800:1.5,\n",
    "      2000:4.,\n",
    "    }\n",
    "    msb=MultiSBWeightsScaler(backgroundclass=-1)\n",
    "    scl=WeightsMultiplier(scales=scales,backgroundclass=-1)\n",
    "    aug=HpMTLBackgroundAugmenter(backgroundclass=-1)\n",
    "    fd=FeatureDivider(\"hpmass\")\n",
    "    wss=WeightedStandardScaler()\n",
    "    steps=[(\"msb\",msb),(\"scl\",scl),(\"aug\",aug),(\"fd\",fd),(\"wss\",wss)]\n",
    "    pipe=PipelineWithWeights(steps)\n",
    "    pipe.fit(X_train,X_train.hpmass, sample_weight=w_train)\n",
    "    #dump(pipe, 'models/massparameterisedNN3_'+region+'_'+datalabel+'_pipe.joblib')\n",
    "    X_tr,y_tr,w_tr=pipe.transform(X_train,y_train, sample_weight=w_train)\n",
    "    X_ts,y_ts,w_ts=pipe.transform(X_test,y_test, sample_weight=w_test)\n",
    "    y_tr=(y_tr>0)\n",
    "    y_ts=(y_ts>0)\n",
    "    print (X_test[])\n",
    "    print (X_test.shape,X_test.mean(), X_test.var())\n",
    "    test1,test2,test3 = wss()\n",
    "    print (capspdasd)\n",
    "    #modelNN3=HpKerasUtils.HpFeedForwardModel(configuration=[64,64],dropout=0.1, verbose=True, input_dim=X_tr.shape[1])\n",
    "    #resultNN3=modelNN3.train((X_tr.values, y_tr.values, w_tr.values),(X_ts.values, y_ts.values, w_ts.values), patience=5,callbacks=getCallbacks(modelNN3))\n",
    "    #arch_file=open('models/massparameterisedNN3_'+region+'_'+datalabel+'_architecture.h5','w')\n",
    "    #arch_file.write(modelNN3.model.to_json())\n",
    "    #arch_file.close()\n",
    "    #modelNN3.model.save_weights('models/massparameterisedNN3_'+region+'_'+datalabel+'_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Region= INC_5j3b invertTestTrain= False H+ mass=all\n",
      "NN3\n",
      "(2979902, 27) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26]\n",
      "(2979902, 27) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26]\n",
      "(2993421, 27) [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26]\n",
      "(378022, 15) nJets                    5.000000\n",
      "nBTags_70                3.000000\n",
      "pT_jet1             207977.306022\n",
      "Mbb_MindR_70        134993.065794\n",
      "pT_jet5              37334.199780\n",
      "H1_all                   0.274585\n",
      "dRbb_avg_70              2.308124\n",
      "dRlepbb_MindR_70         2.213398\n",
      "Muu_MindR_70        154520.414210\n",
      "HT_jets             511592.745957\n",
      "Mbb_MaxPt_70        185193.670672\n",
      "Mbb_MaxM_70         318401.363125\n",
      "Mjjj_MaxPt          294916.721234\n",
      "Centrality_all           0.614468\n",
      "hpmass                 365.748165\n",
      "dtype: float64 nJets               0.000000e+00\n",
      "nBTags_70           0.000000e+00\n",
      "pT_jet1             2.971446e+10\n",
      "Mbb_MindR_70        1.209678e+10\n",
      "pT_jet5             2.123769e+08\n",
      "H1_all              5.638316e-02\n",
      "dRbb_avg_70         2.232738e-01\n",
      "dRlepbb_MindR_70    8.179046e-01\n",
      "Muu_MindR_70        2.644885e+10\n",
      "HT_jets             1.019558e+11\n",
      "Mbb_MaxPt_70        2.644308e+10\n",
      "Mbb_MaxM_70         6.437160e+10\n",
      "Mjjj_MaxPt          4.825599e+10\n",
      "Centrality_all      2.439428e-02\n",
      "hpmass              2.449562e+05\n",
      "dtype: float64\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'capspdasd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-2c46087be711>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;31m#    trainBDTandNN(region, hpmass, invertTestTrain, df_mc)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Region=\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mregion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"invertTestTrain=\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvertTestTrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"H+ mass=all\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mtrainMassParameterisedBDTandNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvertTestTrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-7028f95b4646>\u001b[0m in \u001b[0;36mtrainMassParameterisedBDTandNN\u001b[0;34m(region, invertTestTrain, df_mc)\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0my_ts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_ts\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcapspdasd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m     \u001b[0;31m#modelNN3=HpKerasUtils.HpFeedForwardModel(configuration=[64,64],dropout=0.1, verbose=True, input_dim=X_tr.shape[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m#resultNN3=modelNN3.train((X_tr.values, y_tr.values, w_tr.values),(X_ts.values, y_ts.values, w_ts.values), patience=5,callbacks=getCallbacks(modelNN3))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'capspdasd' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "for region in regions[-1:]:\n",
    "    if region==\"5j\":\n",
    "        df_5j3b=pd.read_hdf('pandas_INC_5j3b.h5', 'INC_5j3b')\n",
    "        df_5j4b=pd.read_hdf('pandas_INC_5jge4b.h5', 'INC_5jge4b')\n",
    "        df_mc=pd.concat([df_5j3b,df_5j4b], ignore_index=False)\n",
    "    else:\n",
    "        df_mc=pd.read_hdf('pandas_'+region+'.h5', region)\n",
    "    for invertTestTrain in [False,True]: #False=2mod0, True=2mod1\n",
    "        #for hpmass in hpmasses:\n",
    "        #    print(\"Region=\",region,\"invertTestTrain=\", invertTestTrain,\"H+ mass=\",hpmass)\n",
    "        #    trainBDTandNN(region, hpmass, invertTestTrain, df_mc)\n",
    "        print(\"Region=\",region,\"invertTestTrain=\", invertTestTrain,\"H+ mass=all\")\n",
    "        trainMassParameterisedBDTandNN(region, invertTestTrain, df_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5j3b.iloc[:,[1,2]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_5j3b.iloc[:,[2,1]].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
